---
layout: single
show_date: true
title: "Atavide: atavistic metagenome analysis with a vision"
author: Rob Edwards
tags: metagenome microbiome annotation 
excerpt: A simple pipeline for complex annotations of metagenomes incorporating lots of tools
---


### Atavide

## What is it?

Atavide is a pipeline that integrates a lot of different tools to annotate and analyze your (random) metagenomics data. The idea is to quickly and easily get you to the hard part: thinking about what your data really means. We have abstracted out all the intermediate analyses so you don't have to do them.

## Who did it?

Atavide is written by Rob and Mike with some helpful input from other friends.

## How do I cite it?

Please see the [citation file](https://github.com/linsalrob/atavide/blob/main/CITATION.md) for the current citation.

# Using atavide

This guide is currently written for using atavide on the Flinders University HPC, deepthought. You will probably need to modify a few things to make it work on another HPC, and we're working on streamlining some of these points and making the installation and running easier.

### Prerequisites

For the Flinders HPC, you will need a few things set up. If you have some of these, feel free to skip that step.
 - [Access to the HPC](https://fame.flinders.edu.au/blog/2020/09/01/deepthought)
 - A [VPN connection](https://fame.flinders.edu.au/blog/2021/09/07/vpn) &#151; but you only need this if you are not at Flinders
 - [conda and snakemake](https://fame.flinders.edu.au/blog/2020/09/02/condadeepthought) installed in your account
 - A [snakemake profile](https://fame.flinders.edu.au/blog/2021/08/02/snakemake-profiles-updated) &#151; just copy Mike's &#151; it's what we all do.
 - If you need pointers, [here are some helpful Linux commands](https://fame.flinders.edu.au/blog/2020/09/01/deepthought)


### Get atavide

Currently, you will need to get a copy of atavide using git:

```bash
# start off in your home directory
cd
# get a copy of atavide
git clone https://github.com/linsalrob/atavide.git
```

This will download all the files that you need to run atavide. Everything else will be downloaded when you start processing your data!


### Your sequence data

You will need some sequence data. Atavide works with compressed or uncompressed fastq files. Put them in a directory called `fastq` in a subdirectory where you will work.

e.g. this set of commands will make the directories for you

```
mkdir --parents sequence_data/fastq
cd sequence_data/fastq
## copy your sequences here
cd ..
```

and now you end up in the directory called `sequence_data` that has one directory called `fastq` and that has your sequence data inside it.


## Make a copy of the settings file

It is good practice to make a copy of the configuration/settings file to your working location. Then, the next time you come to work on this project, you know what was done.

Assuming you are in the directory with the `fastq` directory inside it, e.g. you are in `sequence_data` in the above example, this code will copy the configuration file for you.

```
cp $HOME/atavide/config/process_metagenomes.yaml .
```

**Please note:** You do not need to change any of the basic configuration in this file, and hopefully our defaults make sense.

## Cleaning out host sequences

Atavide includes the option to remove host genome sequences, by adding two additional directives to the `process_metagenomes.yaml` file. If you are interested in this you will 
need a host genome file. Add the following two directives to `process_metagenomes.yaml`:

To the directories data set, add a new option called `host_dbpath` that contains the path to the host database.

Add a new `options` option and add the name of the actual host database you are going to use.

For example, a modified `process_metagenomes.yaml` file might look like:


```bash
# These are the directories where we read the data and store the output
# feel free to change these names
directories:
        Reads: "fastq"
        round1_assembly_output: "assembly.1"
        round1_contig_read_mapping: "reads.contigs.1"
        round2_unassembled_reads: "unassembled_reads"
        round2_assembly_output: "reassembled_reads"
        reads_vs_final_assemblies: "reads_vs_final_assemblies"
        prinseq: "QC"
        statistics: "statistics"
	combined_contig_merging: "final.combined_contigs"
	read_based_annotations: "ReadAnnotations"
        host_dbpath: '/home/edwa0468/hecatomb/databases/human_masked'
options:
        host_dbname: 'human_virus_masked'
```


## Running atavide

To run the pipeline, use the command:

```bash
snakemake -s $HOME/atavide/workflow/process_metagenomes.snakefile --configfile $HOME/atavide/config/process_metagenomes.yaml --profile slurm
```


# What does it do?

`Atavide` runs the following pieces of code for you!

1. Clean the data with [prinseq++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus)
   1a. (Optional) Remove host contamination with [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)

To bin the metagenomes into mags we:
1. Assemble the reads from each sample separately with [megahit](https://github.com/voutcn/megahit)
2. Merge those contigs, and map all the reads from each sample back to the contigs
3. Find reads that have not been mapped
4. Assemble all of the unmapped reads together in one big pile with [megahit](https://github.com/voutcn/megahit)
5. Merge those contigs with the original contigs using [flye](https://github.com/fenderglass/Flye)
6. Map all the reads back to the final contigs and generate a table of read coverages


To calculate read based statistics we:
1. Run [focus](https://github.com/metageni/FOCUS)
2. Run [super-focus](https://github.com/metageni/SUPER-FOCUS/)
3. Generate a super-focus based taxonomy, provided you add an SQL taxonomy file
4. Run [kraken2](https://ccb.jhu.edu/software/kraken2/)
5. Run [singlem](https://github.com/wwood/singlem)



